{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b68799e3-9bfe-4e45-899a-626bc57c5c7e",
   "metadata": {},
   "source": [
    "#\n",
    "### Introduction\n",
    "\n",
    "In this Jupyter Notebook, we aim to clean and prepare a dataset containing information about data science job posts from Glassdoor.It encompasses various industries, job titles,  estimated salaries, Type of ownerships, locations, etc. Our  goal is to perform insightful analysis on the cleaned dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "id": "a79ae833",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "00b20505-1d61-43c1-9ef9-3bf8a7699f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path='/Users/leilajavanmardi/Desktop/Leila/Coding_IronHack/Data_Analytics_Bootcamp/week3/project1/data/raw/Uncleaned_DS_jobs.csv'\n",
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7123198-86f9-43b8-92b0-5c17ec0baeb5",
   "metadata": {},
   "source": [
    "#\n",
    "### Initial Analysis\n",
    "\n",
    "We will begin our analysis by examining the current state of the uncleaned dataset. This initial exploration will provide us with valuable insights into the structure, content, and quality of the data. Through this process, we'll identify any issues or inconsistencies that need to be addressed during the cleaning phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "a068b5dc-f3f3-4bc0-ade8-dd12fac9d014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original data set consists of  672 rows and 15 columns\n",
      "Let's take a closer look at the dataset\n",
      "\n",
      "The following are the first 3 rows of the dataset:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Salary Estimate</th>\n",
       "      <th>Job Description</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Company Name</th>\n",
       "      <th>Location</th>\n",
       "      <th>Headquarters</th>\n",
       "      <th>Size</th>\n",
       "      <th>Founded</th>\n",
       "      <th>Type of ownership</th>\n",
       "      <th>Industry</th>\n",
       "      <th>Sector</th>\n",
       "      <th>Revenue</th>\n",
       "      <th>Competitors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Sr Data Scientist</td>\n",
       "      <td>$137K-$171K (Glassdoor est.)</td>\n",
       "      <td>Description\\n\\nThe Senior Data Scientist is re...</td>\n",
       "      <td>3.1</td>\n",
       "      <td>Healthfirst\\n3.1</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>1001 to 5000 employees</td>\n",
       "      <td>1993</td>\n",
       "      <td>Nonprofit Organization</td>\n",
       "      <td>Insurance Carriers</td>\n",
       "      <td>Insurance</td>\n",
       "      <td>Unknown / Non-Applicable</td>\n",
       "      <td>EmblemHealth, UnitedHealth Group, Aetna</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>$137K-$171K (Glassdoor est.)</td>\n",
       "      <td>Secure our Nation, Ignite your Future\\n\\nJoin ...</td>\n",
       "      <td>4.2</td>\n",
       "      <td>ManTech\\n4.2</td>\n",
       "      <td>Chantilly, VA</td>\n",
       "      <td>Herndon, VA</td>\n",
       "      <td>5001 to 10000 employees</td>\n",
       "      <td>1968</td>\n",
       "      <td>Company - Public</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>Business Services</td>\n",
       "      <td>$1 to $2 billion (USD)</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>$137K-$171K (Glassdoor est.)</td>\n",
       "      <td>Overview\\n\\n\\nAnalysis Group is one of the lar...</td>\n",
       "      <td>3.8</td>\n",
       "      <td>Analysis Group\\n3.8</td>\n",
       "      <td>Boston, MA</td>\n",
       "      <td>Boston, MA</td>\n",
       "      <td>1001 to 5000 employees</td>\n",
       "      <td>1981</td>\n",
       "      <td>Private Practice / Firm</td>\n",
       "      <td>Consulting</td>\n",
       "      <td>Business Services</td>\n",
       "      <td>$100 to $500 million (USD)</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index          Job Title               Salary Estimate  \\\n",
       "0      0  Sr Data Scientist  $137K-$171K (Glassdoor est.)   \n",
       "1      1     Data Scientist  $137K-$171K (Glassdoor est.)   \n",
       "2      2     Data Scientist  $137K-$171K (Glassdoor est.)   \n",
       "\n",
       "                                     Job Description  Rating  \\\n",
       "0  Description\\n\\nThe Senior Data Scientist is re...     3.1   \n",
       "1  Secure our Nation, Ignite your Future\\n\\nJoin ...     4.2   \n",
       "2  Overview\\n\\n\\nAnalysis Group is one of the lar...     3.8   \n",
       "\n",
       "          Company Name       Location  Headquarters                     Size  \\\n",
       "0     Healthfirst\\n3.1   New York, NY  New York, NY   1001 to 5000 employees   \n",
       "1         ManTech\\n4.2  Chantilly, VA   Herndon, VA  5001 to 10000 employees   \n",
       "2  Analysis Group\\n3.8     Boston, MA    Boston, MA   1001 to 5000 employees   \n",
       "\n",
       "   Founded        Type of ownership                Industry  \\\n",
       "0     1993   Nonprofit Organization      Insurance Carriers   \n",
       "1     1968         Company - Public  Research & Development   \n",
       "2     1981  Private Practice / Firm              Consulting   \n",
       "\n",
       "              Sector                     Revenue  \\\n",
       "0          Insurance    Unknown / Non-Applicable   \n",
       "1  Business Services      $1 to $2 billion (USD)   \n",
       "2  Business Services  $100 to $500 million (USD)   \n",
       "\n",
       "                               Competitors  \n",
       "0  EmblemHealth, UnitedHealth Group, Aetna  \n",
       "1                                       -1  \n",
       "2                                       -1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Let's now examine the last rows of the dataset.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Salary Estimate</th>\n",
       "      <th>Job Description</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Company Name</th>\n",
       "      <th>Location</th>\n",
       "      <th>Headquarters</th>\n",
       "      <th>Size</th>\n",
       "      <th>Founded</th>\n",
       "      <th>Type of ownership</th>\n",
       "      <th>Industry</th>\n",
       "      <th>Sector</th>\n",
       "      <th>Revenue</th>\n",
       "      <th>Competitors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>669</th>\n",
       "      <td>669</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>$105K-$167K (Glassdoor est.)</td>\n",
       "      <td>Join a thriving company that is changing the w...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>AccessHope</td>\n",
       "      <td>Irwindale, CA</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670</th>\n",
       "      <td>670</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>$105K-$167K (Glassdoor est.)</td>\n",
       "      <td>100 Remote Opportunity As an AINLP Data Scient...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>ChaTeck Incorporated\\n5.0</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "      <td>Santa Clara, CA</td>\n",
       "      <td>1 to 50 employees</td>\n",
       "      <td>-1</td>\n",
       "      <td>Company - Private</td>\n",
       "      <td>Advertising &amp; Marketing</td>\n",
       "      <td>Business Services</td>\n",
       "      <td>$1 to $5 million (USD)</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671</th>\n",
       "      <td>671</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>$105K-$167K (Glassdoor est.)</td>\n",
       "      <td>Description\\n\\nThe Data Scientist will be part...</td>\n",
       "      <td>2.7</td>\n",
       "      <td>1-800-Flowers\\n2.7</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>Carle Place, NY</td>\n",
       "      <td>1001 to 5000 employees</td>\n",
       "      <td>1976</td>\n",
       "      <td>Company - Public</td>\n",
       "      <td>Wholesale</td>\n",
       "      <td>Business Services</td>\n",
       "      <td>$1 to $2 billion (USD)</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     index       Job Title               Salary Estimate  \\\n",
       "669    669  Data Scientist  $105K-$167K (Glassdoor est.)   \n",
       "670    670  Data Scientist  $105K-$167K (Glassdoor est.)   \n",
       "671    671  Data Scientist  $105K-$167K (Glassdoor est.)   \n",
       "\n",
       "                                       Job Description  Rating  \\\n",
       "669  Join a thriving company that is changing the w...    -1.0   \n",
       "670  100 Remote Opportunity As an AINLP Data Scient...     5.0   \n",
       "671  Description\\n\\nThe Data Scientist will be part...     2.7   \n",
       "\n",
       "                  Company Name           Location     Headquarters  \\\n",
       "669                 AccessHope      Irwindale, CA               -1   \n",
       "670  ChaTeck Incorporated\\n5.0  San Francisco, CA  Santa Clara, CA   \n",
       "671         1-800-Flowers\\n2.7       New York, NY  Carle Place, NY   \n",
       "\n",
       "                       Size  Founded  Type of ownership  \\\n",
       "669                      -1       -1                 -1   \n",
       "670       1 to 50 employees       -1  Company - Private   \n",
       "671  1001 to 5000 employees     1976   Company - Public   \n",
       "\n",
       "                    Industry             Sector                 Revenue  \\\n",
       "669                       -1                 -1                      -1   \n",
       "670  Advertising & Marketing  Business Services  $1 to $5 million (USD)   \n",
       "671                Wholesale  Business Services  $1 to $2 billion (USD)   \n",
       "\n",
       "    Competitors  \n",
       "669          -1  \n",
       "670          -1  \n",
       "671          -1  "
      ]
     },
     "execution_count": 474,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape=df.shape\n",
    "print(f'The original data set consists of  {shape[0]} rows and {shape[1]} columns')\n",
    "print(\"Let's take a closer look at the dataset\")\n",
    "print('\\nThe following are the first 3 rows of the dataset:\\n')\n",
    "display(df.head(3))\n",
    "print(\"\\nLet's now examine the last rows of the dataset.\\n\")\n",
    "df.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "08251d14-5874-4800-8391-29507d1441ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['index', 'Job Title', 'Salary Estimate', 'Job Description', 'Rating',\n",
      "       'Company Name', 'Location', 'Headquarters', 'Size', 'Founded',\n",
      "       'Type of ownership', 'Industry', 'Sector', 'Revenue', 'Competitors'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "col_df=df.columns\n",
    "print(col_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "7f98c267-e699-4145-aca4-ae992cd045f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the type of the column index is: int64\n",
      " the type of the column Job Title is: object\n",
      " the type of the column Salary Estimate is: object\n",
      " the type of the column Job Description is: object\n",
      " the type of the column Rating is: float64\n",
      " the type of the column Company Name is: object\n",
      " the type of the column Location is: object\n",
      " the type of the column Headquarters is: object\n",
      " the type of the column Size is: object\n",
      " the type of the column Founded is: int64\n",
      " the type of the column Type of ownership is: object\n",
      " the type of the column Industry is: object\n",
      " the type of the column Sector is: object\n",
      " the type of the column Revenue is: object\n",
      " the type of the column Competitors is: object\n"
     ]
    }
   ],
   "source": [
    "type=df.dtypes\n",
    "for col in col_df:\n",
    "    print(f' the type of the column {col} is: {type.loc[col]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "id": "dd66d612-5b8a-48e6-96b5-cbc7609a12f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index                672\n",
       "Job Title            172\n",
       "Salary Estimate       30\n",
       "Job Description      489\n",
       "Rating                32\n",
       "Company Name         432\n",
       "Location             207\n",
       "Headquarters         229\n",
       "Size                   9\n",
       "Founded              103\n",
       "Type of ownership     13\n",
       "Industry              58\n",
       "Sector                23\n",
       "Revenue               14\n",
       "Competitors          108\n",
       "dtype: int64"
      ]
     },
     "execution_count": 477,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "id": "82d65979-1092-4100-8f5f-32492d2ca6bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the missing values of the column index : 0\n",
      " the missing values of the column Job Title : 0\n",
      " the missing values of the column Salary Estimate : 0\n",
      " the missing values of the column Job Description : 0\n",
      " the missing values of the column Rating : 0\n",
      " the missing values of the column Company Name : 0\n",
      " the missing values of the column Location : 0\n",
      " the missing values of the column Headquarters : 0\n",
      " the missing values of the column Size : 0\n",
      " the missing values of the column Founded : 0\n",
      " the missing values of the column Type of ownership : 0\n",
      " the missing values of the column Industry : 0\n",
      " the missing values of the column Sector : 0\n",
      " the missing values of the column Revenue : 0\n",
      " the missing values of the column Competitors : 0\n"
     ]
    }
   ],
   "source": [
    "null_values_origin=df.isna().sum()\n",
    "for col in col_df:\n",
    "    print(f' the missing values of the column {col} : {null_values_origin.loc[col]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af48705-5acf-4858-95cf-b483d7f4d01b",
   "metadata": {},
   "source": [
    "#####\n",
    "#### Relevant Data Quality Issues\n",
    "\n",
    "As observed, the uncleaned dataset encompasses several columns containing values in string, integer, and float formats. The most relevant data quality issues and key characteristics are:\n",
    "\n",
    "- __Mixed Case:__ The values exhibit mixed cases, including uppercase, lowercase, and title case formats.\n",
    "- __Inconsistent Naming Conventions:__ Inconsistencies are observed in the entered values, such as the usage of abbreviations or acronyms.\n",
    "- __Variability in Terminology:__ Different values may describe similar content using varying terminology, leading to potential inconsistencies.\n",
    "- __Special Characters and Numbers:__ Some values include special characters, numbers, or additional information alongside the main content.\n",
    "- __Potential Errors and Non-Relevant Information:__ Some entries include irrelevant details or information that do not contribute to the dataset's intended analysis\n",
    "- __Placeholder Values__: At first glance, the dataset appears to have no missing values. However, upon closer inspection, it becomes evident that missing values are represented by the placeholder '-1'. Addressing this is essential to accurately represent missing data and ensure data integrity, especially for our numerical columns such as rating and founded.\n",
    "\n",
    "While a comprehensive analysis of the entire dataset has been conducted, only a few samples are displayed below to illustrate the points mentioned above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "656f3448-45bd-421c-839c-e71b077bac54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Industry\n",
       "-1                                          71\n",
       "Biotech & Pharmaceuticals                   66\n",
       "IT Services                                 61\n",
       "Computer Hardware & Software                57\n",
       "Aerospace & Defense                         46\n",
       "Enterprise Software & Network Solutions     43\n",
       "Consulting                                  38\n",
       "Staffing & Outsourcing                      36\n",
       "Insurance Carriers                          28\n",
       "Internet                                    27\n",
       "Advertising & Marketing                     23\n",
       "Health Care Services & Hospitals            21\n",
       "Research & Development                      17\n",
       "Federal Agencies                            16\n",
       "Investment Banking & Asset Management       13\n",
       "Banks & Credit Unions                        8\n",
       "Lending                                      8\n",
       "Energy                                       5\n",
       "Consumer Products Manufacturing              5\n",
       "Telecommunications Services                  5\n",
       "Insurance Agencies & Brokerages              4\n",
       "Food & Beverage Manufacturing                4\n",
       "Utilities                                    3\n",
       "Electrical & Electronic Manufacturing        3\n",
       "Colleges & Universities                      3\n",
       "Other Retail Stores                          3\n",
       "Architectural & Engineering Services         3\n",
       "Chemical Manufacturing                       3\n",
       "Real Estate                                  3\n",
       "Miscellaneous Manufacturing                  3\n",
       "Accounting                                   3\n",
       "Wholesale                                    3\n",
       "Industrial Manufacturing                     3\n",
       "Video Games                                  3\n",
       "Travel Agencies                              2\n",
       "Express Delivery Services                    2\n",
       "Timber Operations                            2\n",
       "Financial Transaction Processing             2\n",
       "Construction                                 2\n",
       "Health, Beauty, & Fitness                    2\n",
       "Transportation Equipment Manufacturing       2\n",
       "Oil & Gas Services                           2\n",
       "Venture Capital & Private Equity             2\n",
       "Consumer Electronics & Appliances Stores     2\n",
       "Department, Clothing, & Shoe Stores          1\n",
       "Publishing                                   1\n",
       "Social Assistance                            1\n",
       "Farm Support Services                        1\n",
       "Logistics & Supply Chain                     1\n",
       "Transportation Management                    1\n",
       "State & Regional Agencies                    1\n",
       "Hotels, Motels, & Resorts                    1\n",
       "Food & Beverage Stores                       1\n",
       "News Outlet                                  1\n",
       "Telecommunications Manufacturing             1\n",
       "Cable, Internet & Telephone Providers        1\n",
       "Shipping                                     1\n",
       "Rail                                         1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 479,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Industry.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "7585d975-b528-4ccd-b2ea-4372c9e9f014",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Sr Data Scientist', 'Data Scientist',\n",
       "       'Data Scientist / Machine Learning Expert',\n",
       "       'Staff Data Scientist - Analytics',\n",
       "       'Data Scientist - Statistics, Early Career', 'Data Modeler',\n",
       "       'Experienced Data Scientist', 'Data Scientist - Contract',\n",
       "       'Data Analyst II', 'Medical Lab Scientist',\n",
       "       'Data Scientist/Machine Learning', 'Human Factors Scientist',\n",
       "       'Business Intelligence Analyst I- Data Insights',\n",
       "       'Data Scientist - Risk', 'Data Scientist-Human Resources',\n",
       "       'Senior Research Statistician- Data Scientist', 'Data Engineer',\n",
       "       'Associate Data Scientist', 'Business Intelligence Analyst',\n",
       "       'Senior Analyst/Data Scientist', 'Data Analyst',\n",
       "       'Machine Learning Engineer', 'Data Analyst I',\n",
       "       'Scientist - Molecular Biology',\n",
       "       'Computational Scientist, Machine Learning',\n",
       "       'Senior Data Scientist', 'Jr. Data Engineer',\n",
       "       'E-Commerce Data Analyst', 'Data Analytics Engineer',\n",
       "       'Product Data Scientist - Ads Data Science',\n",
       "       'Data Scientist - Intermediate', 'Global Data Analyst',\n",
       "       'Data & Machine Learning Scientist',\n",
       "       'Data Scientist - Machine Learning', 'Data Engineer (Remote)',\n",
       "       'Data Scientist, Applied Machine Learning - Bay Area',\n",
       "       'Principal Data Scientist', 'Business Data Analyst',\n",
       "       'Purification Scientist', 'Data Engineer, Enterprise Analytics',\n",
       "       'Data Scientist 3 (718)', 'Real World Science, Data Scientist',\n",
       "       'Data Scientist - Image and Video Analytics',\n",
       "       'Data Science Manager, Payment Acceptance - USA',\n",
       "       'Data Scientist / Applied Mathematician',\n",
       "       'Patient Safety- Associate Data Scientist',\n",
       "       '(Sr.) Data Scientist -', 'Data Scientist, Kinship - NYC/Portland',\n",
       "       'Applied Technology Researcher / Data Scientist',\n",
       "       'Health Data Scientist - Biomedical/Biostats',\n",
       "       'Staff Data Scientist', 'Sr Data Engineer (Sr BI Developer)',\n",
       "       'Lead Data Scientist', 'RFP Data Analyst',\n",
       "       'Data Scientist (TS/SCI)', 'Software Engineer - Data Science',\n",
       "       'Data Analyst/Engineer', 'NGS Scientist', 'Senior Data Engineer',\n",
       "       'Sr. ML/Data Scientist - AI/NLP/Chatbot',\n",
       "       'Data Integration and Modeling Engineer',\n",
       "       'Tableau Data Engineer 20-0117', 'AI Data Scientist',\n",
       "       'Research Scientist Patient Preferences (Remote)',\n",
       "       'Scientist - Biomarker and Flow Cytometry', 'Analytics Manager',\n",
       "       'Staff Scientist- Upstream PD',\n",
       "       'Sr Scientist - Extractables & Leachables',\n",
       "       'ELISA RESEARCH SCIENTIST (CV-15)', 'Say Business Data Analyst',\n",
       "       'Geospatial Data Scientist', 'Computational Scientist',\n",
       "       'Senior Data Analyst', 'Sr Data Analyst',\n",
       "       'Machine Learning Scientist - Bay Area, CA',\n",
       "       'Senior Data Scientist - Algorithms',\n",
       "       'Senior Data & Machine Learning Scientist',\n",
       "       'Research Scientist - Patient-Centered Research (Remote)',\n",
       "       'Jr. Business Data Analyst (position added 6/12/2020)',\n",
       "       'Sr. Data Scientist II',\n",
       "       'Production Engineer - Statistics/Data Analysis',\n",
       "       'Statistical Scientist', 'Computational Behavioral Scientist',\n",
       "       'Principal Data Scientist - Machine Learning',\n",
       "       'Principal Machine Learning Scientist',\n",
       "       'Senior Data Scientist - R&D Oncology',\n",
       "       'Health Plan Data Analyst, Sr',\n",
       "       'Principal Scientist/Associate Director, Quality Control and Analytical Technologies',\n",
       "       'Analytics - Business Assurance Data Analyst',\n",
       "       'Senior Data Scientist – Image Analytics, Novartis AI Innovation Lab',\n",
       "       'Data Science Instructor', 'Senior Business Intelligence Analyst',\n",
       "       'In-Line Inspection Data Analyst',\n",
       "       'Data Scientist - TS/SCI FSP or CI Required',\n",
       "       'Data Scientist - TS/SCI Required',\n",
       "       'Data Science Software Engineer',\n",
       "       'ENGINEER - COMPUTER SCIENTIST - RESEARCH COMPUTER SCIENTIST - SIGNAL PROCESSING - SAN ANTONIO OR',\n",
       "       'AI Ops Data Scientist', 'Intelligence Data Analyst, Senior',\n",
       "       'Analytics Manager - Data Mart',\n",
       "       'Data Modeler (Analytical Systems)',\n",
       "       'Senior Machine Learning Scientist - Bay Area, CA',\n",
       "       'Report Writer-Data Analyst', 'Staff Data Scientist - Pricing',\n",
       "       'Equity Data Insights Analyst - Quantitative Analyst',\n",
       "       'Operations Data Analyst', 'Software Data Engineer',\n",
       "       'Real World Evidence (RWE) Scientist', 'Computer Scientist 1',\n",
       "       'Environmental Data Science', 'Staff BI and Data Engineer',\n",
       "       'Data Scientist - Statistics, Mid-Career',\n",
       "       'Director of Data Science',\n",
       "       'Data Engineer, Digital & Comp Pathology',\n",
       "       'Manager / Lead, Data Science & Analytics',\n",
       "       'Diversity and Inclusion Data Analyst',\n",
       "       'Data Scientist Machine Learning', 'Chief Scientist',\n",
       "       'Development Scientist, Voltaren',\n",
       "       'Principal Data & Analytics Platform Engineer',\n",
       "       'Machine Learning Engineer/Scientist',\n",
       "       'Data Analyst - Unilever Prestige', 'VP, Data Science',\n",
       "       'Data Engineer - Kafka', 'Decision Scientist',\n",
       "       'Data Science All Star Program - Data Engineer Track',\n",
       "       'Scientist - Machine Learning', 'Sr. Data Scientist',\n",
       "       'Applied AI Scientist / Engineer',\n",
       "       'Data Engineer (Analytics, SQL, Python, AWS)',\n",
       "       'Senior Data Analyst - Finance & Platform Analytics',\n",
       "       'Market Research Data Scientist',\n",
       "       'IT Partner Digital Health Technology and Data Science',\n",
       "       'Software Engineer (Data Scientist, C,C++,Linux,Unix) - SISW - MG',\n",
       "       'Senior Clinical Data Scientist Programmer',\n",
       "       'Computer Vision / Deep Learning Scientist',\n",
       "       'Data Solutions Engineer - Data Modeler',\n",
       "       'Data Scientist (TS/SCI w/ Poly)',\n",
       "       'Weapons and Sensors Engineer/Scientist',\n",
       "       'Applied Computer Scientist', 'Cloud Data Engineer (Azure)',\n",
       "       'Lead Certified Clinical Laboratory Scientist - Saturday - Tuesday, 8:00pm - 6:30am shift',\n",
       "       'Sr. Data Analyst',\n",
       "       'Senior Scientist - Toxicologist - Product Integrity (Stewardship)',\n",
       "       'Senior Machine Learning Engineer',\n",
       "       'Data Scientist- Industrial Discrete Sector Industry',\n",
       "       'Senior Principal Data Scientist (Python/R)',\n",
       "       'Data Scientist(s)/Machine Learning Engineer',\n",
       "       'Scientist / Group Lead, Cancer Biology',\n",
       "       'Manager, Field Application Scientist, Southeast',\n",
       "       'COMPUTER SCIENTIST - ENGINEER - RESEARCH COMPUTER SCIENTIST - SIGNAL PROCESSING',\n",
       "       'Machine Learning Scientist / Engineer', 'Data Science Analyst',\n",
       "       'COMPUTER SCIENTIST - ENGINEER - RESEARCH COMPUTER SCIENTIST - TRANSPORTATION TECHNOLOGY',\n",
       "       'Software Engineer - Machine Learning & Data Science (Applied Intelligence Services Team)',\n",
       "       'Clinical Data Analyst', 'Data Scientist Technical Specialist',\n",
       "       'Data Science Manager', 'Big Data Engineer', 'Data Architect',\n",
       "       'Aviation AI/ML Data Scientist', 'Machine Learning Engineer, Sr.',\n",
       "       'Information Systems Engineering Specialist (Engineering Scientist)',\n",
       "       'Scientist/Research Associate-Metabolic Engineering',\n",
       "       'Vice President, Biometrics and Clinical Data Management',\n",
       "       'Enterprise Data Analyst (Enterprise Portfolio Management Office)',\n",
       "       'Lead Data Scientist – Network Analysis and Control',\n",
       "       'Sr. Research Associate/ Scientist, NGS prep & Molecular Genomics',\n",
       "       'Developer III - Data Science',\n",
       "       'Hydrogen/Tritium Materials Scientist (Experienced)',\n",
       "       'Data Scientist/Data Analytics Practitioner',\n",
       "       'AI/ML - Machine Learning Scientist, Siri Understanding'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Job Title'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "id": "e56ea150-d458-4cee-9a91-01ad8089a757",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Insurance Carriers', 'Research & Development', 'Consulting',\n",
       "       'Electrical & Electronic Manufacturing', 'Advertising & Marketing',\n",
       "       'Computer Hardware & Software', 'Biotech & Pharmaceuticals',\n",
       "       'Consumer Electronics & Appliances Stores',\n",
       "       'Enterprise Software & Network Solutions', 'IT Services', 'Energy',\n",
       "       'Chemical Manufacturing', 'Federal Agencies', 'Internet',\n",
       "       'Health Care Services & Hospitals',\n",
       "       'Investment Banking & Asset Management', 'Aerospace & Defense',\n",
       "       'Utilities', '-1', 'Express Delivery Services',\n",
       "       'Staffing & Outsourcing', 'Insurance Agencies & Brokerages',\n",
       "       'Consumer Products Manufacturing', 'Industrial Manufacturing',\n",
       "       'Food & Beverage Manufacturing', 'Banks & Credit Unions',\n",
       "       'Video Games', 'Shipping', 'Telecommunications Services',\n",
       "       'Lending', 'Cable, Internet & Telephone Providers', 'Real Estate',\n",
       "       'Venture Capital & Private Equity', 'Miscellaneous Manufacturing',\n",
       "       'Oil & Gas Services', 'Transportation Equipment Manufacturing',\n",
       "       'Telecommunications Manufacturing', 'Transportation Management',\n",
       "       'News Outlet', 'Architectural & Engineering Services',\n",
       "       'Food & Beverage Stores', 'Other Retail Stores',\n",
       "       'Hotels, Motels, & Resorts', 'State & Regional Agencies',\n",
       "       'Financial Transaction Processing', 'Timber Operations',\n",
       "       'Colleges & Universities', 'Travel Agencies', 'Accounting',\n",
       "       'Logistics & Supply Chain', 'Farm Support Services',\n",
       "       'Social Assistance', 'Construction',\n",
       "       'Department, Clothing, & Shoe Stores', 'Publishing',\n",
       "       'Health, Beauty, & Fitness', 'Wholesale', 'Rail'], dtype=object)"
      ]
     },
     "execution_count": 481,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Industry.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "id": "c3d7e99c-1cdf-4cc6-a8d3-850f11592da3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Series.unique of 0      $137K-$171K (Glassdoor est.)\n",
       "1      $137K-$171K (Glassdoor est.)\n",
       "2      $137K-$171K (Glassdoor est.)\n",
       "3      $137K-$171K (Glassdoor est.)\n",
       "4      $137K-$171K (Glassdoor est.)\n",
       "                   ...             \n",
       "667    $105K-$167K (Glassdoor est.)\n",
       "668    $105K-$167K (Glassdoor est.)\n",
       "669    $105K-$167K (Glassdoor est.)\n",
       "670    $105K-$167K (Glassdoor est.)\n",
       "671    $105K-$167K (Glassdoor est.)\n",
       "Name: Salary Estimate, Length: 672, dtype: object>"
      ]
     },
     "execution_count": 482,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Salary Estimate'].unique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc46822-ebfd-4021-8049-1d634463d022",
   "metadata": {},
   "source": [
    "#####\n",
    "### Data Cleaning Process\n",
    "Following the identification of relevant data quality issues and key characteristics, the next step involves cleaning the data to address these issues and ensure its suitability for analysis. In this section, I will outline the steps taken to clean the dataset using methods such as regular expressions (regex) and tailored functions for specific columns. These strategies were employed to remove inconsistencies, standardize formats, and address missing or irrelevant information efficiently. The cleaning process aims to enhance the quality and integrity of the dataset, laying the groundwork for accurate and reliable analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "id": "3db24925-0169-45c6-a62c-677b6844be1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After uniformating column names and removing duplicates, the data set has 672 rows and 15 columns\n"
     ]
    }
   ],
   "source": [
    "df_clean=general_cleaning(df)\n",
    "df_clean.shape\n",
    "print(f'After uniformating column names and removing duplicates, the data set has {df_clean.shape[0]} rows and {df_clean.shape[1]} columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30771eda-236a-417a-90f6-44f5df7181b2",
   "metadata": {},
   "source": [
    "#####\n",
    "#### Placeholder: Replacing -1 with numpy.nan\n",
    "Given that the dataset is about data science job postings on Glassdoor and -1 values represent missing data, in the first step, we replace -1 with numpy.nan. Such a strategy appears to be a suitable first step considering the context and objectives of the analysis and gives us a good glance at the amount of missing values present.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "id": "8520580f-33d6-44b9-8a7e-1fa2fe5126bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " To provide the missing indicator, the -1 values are replaced with numpy.nan\n"
     ]
    }
   ],
   "source": [
    "print(' To provide the missing indicator, the -1 values are replaced with numpy.nan')\n",
    "df_clean.replace({-1:np.nan,'-1': np.nan}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "id": "9a46b919-c41f-4baf-8ecc-404cea4a9b0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the missing values of the column index : 0\n",
      " the missing values of the column job_title : 0\n",
      " the missing values of the column salary_estimate : 0\n",
      " the missing values of the column job_description : 0\n",
      " the missing values of the column rating : 50\n",
      " the missing values of the column company_name : 0\n",
      " the missing values of the column location : 0\n",
      " the missing values of the column headquarters : 31\n",
      " the missing values of the column size : 27\n",
      " the missing values of the column founded : 118\n",
      " the missing values of the column type_of_ownership : 27\n",
      " the missing values of the column industry : 71\n",
      " the missing values of the column sector : 71\n",
      " the missing values of the column revenue : 27\n",
      " the missing values of the column competitors : 501\n"
     ]
    }
   ],
   "source": [
    "columns=df_clean.columns\n",
    "null_values=df_clean.isna().sum()\n",
    "for col in columns:\n",
    "    print(f' the missing values of the column {col} : {null_values.loc[col]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ebdd70-255f-483c-b4aa-4473eb7f3705",
   "metadata": {},
   "source": [
    "After replacing the placeholder '-1' with numpy.nan, there are missing values present across multiple columns in the dataset. Notably, the <b>\"competitors\"</b> exhibits a notable number of missing values (501), followed by <b>\"founded\"</b> (118).\n",
    "This observation underscores the importance of preprocessing to ensure accuracy in subsequent analyses. By effectively handling missing values in the cleaning process, we can enhance the robustness of our analysis, enabling more accurate interpretations. In the next steps, it will be decided for each column separately how the missing values should be filled.\n",
    "On the other hand <b>job_title, salary_estimate, company_name</b> and <b>location</b> columns have <b>no missing values</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddcc861-1893-4c8a-bd37-f509061bd207",
   "metadata": {},
   "source": [
    "#####\n",
    "#### Tailored functions for cleaing the columns\n",
    "The data cleaning process for the job title, industry, salary, location, headquarters, and company name columns involved several steps to standardize and normalize the values for consistency and ease of analysis. Initially, the unique values and their frequency counts were examined. Next, the uncleaned dataset was analyzed to identify common patterns and variations within each column. Subsequently, custom cleaning functions were developed for each column to effectively address these variations and reduce the number of unique values. Throughout this process, we ensured uniformity and removed any irrelevant characters.<br>\n",
    "In the next step, a dictionary of patterns was defined, where keys represent regular expression patterns matching the original content of each column, and values represent the corresponding standardized and uniformed content. When addressing missing values and irrelevant data, for non-numeric columns, any unmatched values are categorized as 'others'. However, for numeric columns, missing values were individually replaced to ensure data integrity and accuracy.<br>\n",
    "This cleaning strategy aims to standardize the data by identifying common patterns and replacing them with predefined standardized values to handle various variations. By doing so, it ensures a reduction in the number of unique values and enables consistency in the representation of values within each column. This, in turn, makes it easier to analyze and interpret the data accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "id": "bf34a14f-be06-485d-abb6-9d3a28256861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In the original dataset the job titles had \u001b[1m 172\u001b[0m uniqe values\n",
      "After uniformating and recategorizing the Jobs titles and handling the irrelevant information,\n",
      "we were able to reduce the amount of the unique values.\n",
      "\n",
      "The job titles are now categorized in the follwoing \u001b[1m8\u001b[0m groups:\n",
      "['data_scientist' 'others' 'data_analyst' 'data_engineer'\n",
      " 'machine_learning_engineer' 'senior_data_scientist'\n",
      " 'data_science_analytics_leadership' 'computational_scientist']\n"
     ]
    }
   ],
   "source": [
    "df_clean['job_cleaned']=df_clean.job_title.apply(cleaning_job_title)\n",
    "print(f\"\\nIn the original dataset the job titles had \\033[1m {df['Job Title'].nunique()}\\033[0m uniqe values\")\n",
    "print(f'After uniformating and recategorizing the Jobs titles and handling the irrelevant information,\\nwe were able to reduce the amount of the unique values.\\n\\nThe job titles are now categorized in the follwoing \\033[1m{df_clean.job_cleaned.nunique()}\\033[0m groups:')\n",
    "print(df_clean.job_cleaned.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "id": "402d6a31-a3e7-4312-963e-b08e25142d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In the original dataset the industry column had \u001b[1m58\u001b[0m uniqe values\n",
      "\n",
      "After uniformating and recategorizing the Jobs titles and handling the irrelevant information,\n",
      "we were able to reduce the amount of the unique values.\n",
      "The job titles are now categorized in the follwoing \u001b[1m16\u001b[0m groups:\n",
      "['insurance_agencies' 'others' 'consulting' 'manufacturing'\n",
      " 'advertising_marketing' 'computer_hardware_software'\n",
      " 'enterprise_software_network_solutions ' 'energy_and_utilities'\n",
      " 'government_public_sector' 'internet_telephone_providers'\n",
      " 'healthcare_and_pharmaceuticals' 'finance_and_banking'\n",
      " 'staffing_outsourcing' 'transportation_logistics'\n",
      " 'telecommunications_services' 'real_estate_construction']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_clean['indu_cl'] = df_clean['industry'].apply(cleaning_industry)\n",
    "print(f\"\\nIn the original dataset the industry column had \\033[1m{df['Industry'].nunique()}\\033[0m uniqe values\")\n",
    "print(f'\\nAfter uniformating and recategorizing the Jobs titles and handling the irrelevant information,\\nwe were able to reduce the amount of the unique values.\\nThe job titles are now categorized in the follwoing \\033[1m{df_clean.indu_cl.nunique()}\\033[0m groups:')\n",
    "print(f'{df_clean.indu_cl.unique()}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39aaf01-6b8f-4bea-b3bf-961837405d80",
   "metadata": {},
   "source": [
    "\n",
    "##### Cleanign the locations and the headquarters\n",
    "Initially, my approach involved removing the abbreviations present in each value by splitting the values and selecting index 0 (the code for this is marked in the function as a comment). However, upon closer inspection, I realized that these abbreviations correspond to different geographical locations. For instance, \"Columbia, MD\" refers to Columbia, Maryland, \"Columbia, MO\" to Columbia, Missouri, and \"Columbia, SC\" to Columbia, South Carolina.\n",
    "\n",
    "Therefore, I made the decision not to remove the abbreviations. An alternative strategy could have been to remove the abbreviations and rename locations with the same name but different geographical identifiers. However, this approach did not directly impact the analysis, and it would not have reduced the number of unique values. Moreover, accurately assigning the abbreviations without explicit information would have been challenging, requiring additional research to understand which abbreviation corresponds to which geographical location. As a result, the abbreviations remained in the final dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "id": "2c3e493b-d892-48c6-a846-112dd4f3809d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "the location had \u001b[1m0\u001b[0m missing values in the original dataset.\n",
      "After the cleanign process,the location column has \u001b[1m207\u001b[0m unique values.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_clean['location_cleaned']=df_clean.location.apply(cleaning_locations)\n",
    "print(f'\\nthe location had \\033[1m{df.Location.isna().sum()}\\033[0m missing values in the original dataset.')\n",
    "print(f'After the cleanign process,the location column has \\033[1m{df_clean.location_cleaned.nunique()}\\033[0m unique values.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "id": "c9a13a21-125c-4ffe-9de3-a3ab4d37b16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['headquarters_cleaned']=df_clean.headquarters.apply(cleaning_headquarters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2297765-d123-42bc-8c18-137d21164cae",
   "metadata": {},
   "source": [
    "##### Cleanign the salary estimates\n",
    "The cleaning process for the salary column involves several steps aimed at standardizing and normalizing the salary values. Initially, all values are cast to strings to ensure uniformity. Next, the '$' and 'K' symbols are removed to clean the formatting. Following this, any parentheses and spaces are removed from the string using regex substitution.\n",
    "<br>Subsequently, the process involves identifying and replacing repeated patterns within the salary values. This is achieved using a predefined dictionary containing regular expressions as keys and their corresponding replacement values. For instance, patterns such as '56-97', '66-112', '69-116', '71-123', or '79-106' are replaced with '56-125', representing a broader salary range.\n",
    "The function is designed to assigns the missing data as 'not_verified' however, there were no missing values in this column. Overall, the cleaning process ensures consistency in the representation of salary values and reduces the unique values making it easier to analyze and interpret the data accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "id": "bb5d0ba5-5fe9-4ea2-9b1e-435baa851e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In the original dataset the salaries had \u001b[1m30\u001b[0m uniqe values\n",
      "After uniformating,the salaries are now categorized in the follwoing \u001b[1m7\u001b[0m groups:\n",
      "['120-200' '75-145' '90-170' '56-125' '140-225' '30-56' '210-335']\n"
     ]
    }
   ],
   "source": [
    "df_clean['sal_cleaned']=df_clean.salary_estimate.map(cleaning_salary)\n",
    "print(f\"\\nIn the original dataset the salaries had \\033[1m{df['Salary Estimate'].nunique()}\\033[0m uniqe values\")\n",
    "print(f'After uniformating,the salaries are now categorized in the follwoing \\033[1m{df_clean.sal_cleaned.nunique()}\\033[0m groups:')\n",
    "print(df_clean.sal_cleaned.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "id": "4ad8840b-9ff7-425a-ab7f-30473520e4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After uniformating, the reveneus are now categorized in the bellow \u001b[1m13\u001b[0m groups,\n",
      "while the missing or nonapplicable values were assigned as unknown / non-applicable:\n",
      "['unknown_non_applicable' '$1 to $2 billion ' '$100 to $500 million '\n",
      " '$10+ billion ' '$2 to $5 billion ' '$500 million to $1 billion '\n",
      " '$5 to $10 billion ' '$10 to $25 million ' '$25 to $50 million '\n",
      " '$50 to $100 million ' '$1 to $5 million ' '$5 to $10 million '\n",
      " 'less than $1 million ']\n"
     ]
    }
   ],
   "source": [
    "df_clean['revenue_cleaned']=df_clean.revenue.str.lower().str.strip().str.replace('(usd)','')\n",
    "df_clean.revenue_cleaned = df_clean.revenue_cleaned.str.replace('unknown / non-applicable','unknown_non_applicable')\n",
    "df_clean.revenue_cleaned =df_clean.revenue_cleaned.fillna('unknown_non_applicable')\n",
    "print(f'\\nAfter uniformating, the reveneus are now categorized in the bellow \\033[1m{df_clean.revenue_cleaned.nunique()}\\033[0m groups,\\nwhile the missing or nonapplicable values were assigned as unknown / non-applicable:')\n",
    "print(df_clean.revenue_cleaned.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "id": "4c1b2aaf-0d2c-4ce9-af96-0f5dfaac809d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "df_clean['size_cleaned']= df_clean['size'].fillna('unknown').replace('Unknown','unknown')\n",
    "print(df_clean['size_cleaned'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "id": "434451f5-cd9b-4b2d-a1af-c4949cca11a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['company_name_cleaned']=df_clean.company_name.apply(cleaning_companies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "id": "b5a85aaf-1091-400f-8f4d-7a12e6dffd4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After uniformating, the sectors are now categorized in the follwoing \u001b[1m23\u001b[0m groups.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_clean.sector.unique()\n",
    "df_clean['sector_cleaned']=df_clean.sector.str.lower().str.strip().str.replace(' & ','_&_')\n",
    "df_clean.sector_cleaned = df_clean.sector_cleaned.fillna('unknown')\n",
    "print(f'\\nAfter uniformating, the sectors are now categorized in the follwoing \\033[1m{df_clean.sector_cleaned.nunique()}\\033[0m groups.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "id": "63fa89ef-f3a5-496e-84e2-87d77afb7941",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['rating_cleaned'] = df_clean.rating.astype(str)\n",
    "df_clean.rating_cleaned = df_clean.rating_cleaned.str.strip()\n",
    "df_clean.rating_cleaned.unique()\n",
    "df_clean['rating_cleaned']=df_clean.rating_cleaned.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "id": "7950695d-1059-4559-931e-b298dbce0301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nonprofit_organization' 'company_public' 'private_practice_firm'\n",
      " 'company_private' 'government' 'subsidiary_or_business_segment'\n",
      " 'other_organization' 'unknown' 'hospital' 'self-employed'\n",
      " 'college_university' 'contract']\n"
     ]
    }
   ],
   "source": [
    "df_clean['type_of_ownership_cleaned']=df_clean.type_of_ownership.str.lower().str.strip().str.replace(' - ','_').str.replace(' / ','_').str.replace(' ','_')\n",
    "df_clean['type_of_ownership_cleaned']=df_clean['type_of_ownership_cleaned'].fillna('unknown')\n",
    "print(df_clean.type_of_ownership_cleaned.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "id": "f15eb0e7-cff1-4452-99ab-72f5f52a8b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118\n",
      "[1993. 1968. 1981. 2000. 1998. 2010. 1996. 1990. 1983. 2014. 2012. 2016.\n",
      " 1965. 1973. 1986. 1997. 2015. 1945. 1988. 2017. 2011. 1967. 1860. 1992.\n",
      " 2003. 1951. 2005. 2019. 1925. 2008. 1999. 1978. 1966. 1912. 1958. 2013.\n",
      " 1849. 1781. 1926. 2006. 1994. 1863. 1995.   nan 1982. 1974. 2001. 1985.\n",
      " 1913. 1971. 1911. 2009. 1959. 2007. 1939. 2002. 1961. 1963. 1969. 1946.\n",
      " 1957. 1953. 1948. 1850. 1851. 2004. 1976. 1918. 1954. 1947. 1955. 2018.\n",
      " 1937. 1917. 1935. 1929. 1820. 1952. 1932. 1894. 1960. 1788. 1830. 1984.\n",
      " 1933. 1880. 1887. 1970. 1942. 1980. 1989. 1908. 1853. 1875. 1914. 1898.\n",
      " 1956. 1977. 1987. 1896. 1972. 1949. 1962.]\n",
      "[1993 1968 1981 2000 1998 2010 1996 1990 1983 2014 2012 2016 1965 1973\n",
      " 1986 1997 2015 1945 1988 2017 2011 1967 1860 1992 2003 1951 2005 2019\n",
      " 1925 2008 1999 1978 1966 1912 1958 2013 1849 1781 1926 2006 1994 1863\n",
      " 1995    0 1982 1974 2001 1985 1913 1971 1911 2009 1959 2007 1939 2002\n",
      " 1961 1963 1969 1946 1957 1953 1948 1850 1851 2004 1976 1918 1954 1947\n",
      " 1955 2018 1937 1917 1935 1929 1820 1952 1932 1894 1960 1788 1830 1984\n",
      " 1933 1880 1887 1970 1942 1980 1989 1908 1853 1875 1914 1898 1956 1977\n",
      " 1987 1896 1972 1949 1962]\n"
     ]
    }
   ],
   "source": [
    "print(df_clean.founded.isna().sum())\n",
    "print(df_clean.founded.unique())\n",
    "df_clean['founded_clean']= df_clean.founded.fillna(0).astype(int)\n",
    "print(df_clean.founded_clean.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "id": "9d455893-4c43-49ee-82a8-6238698be7bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['index', 'job_description', 'competitors', 'job_cleaned', 'indu_cl',\n",
      "       'location_cleaned', 'headquarters_cleaned', 'sal_cleaned',\n",
      "       'revenue_cleaned', 'size_cleaned', 'company_name_cleaned',\n",
      "       'sector_cleaned', 'rating_cleaned', 'type_of_ownership_cleaned',\n",
      "       'founded_clean'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "col_to_drop =['job_title', 'industry', 'salary_estimate','location','size', 'headquarters', 'company_name','sector', 'revenue', 'rating','type_of_ownership','founded']\n",
    "df_clean.drop(columns=col_to_drop, inplace=True)\n",
    "print(df_clean.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e393c6e5-9eaf-4b18-a081-8f2b8df86847",
   "metadata": {},
   "source": [
    "#\n",
    "#### Final clean Dataset \n",
    "Initially, in alignment with our research objectives and to address identified issues, two additional columns were removed, followed by a renaming of all remaining columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "id": "f94a553e-8edd-49f2-83a5-66a714da0d4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'job_cleaned', 'indu_cl', 'location_cleaned',\n",
       "       'headquarters_cleaned', 'sal_cleaned', 'revenue_cleaned',\n",
       "       'size_cleaned', 'company_name_cleaned', 'sector_cleaned',\n",
       "       'rating_cleaned', 'type_of_ownership_cleaned', 'founded_clean'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 499,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final=df_clean.drop(columns=['job_description','competitors'])\n",
    "df_final.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "id": "2646aac7-157c-43fe-bb24-7404fc4ba518",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>job_title</th>\n",
       "      <th>salary_range_$_K</th>\n",
       "      <th>location</th>\n",
       "      <th>size</th>\n",
       "      <th>founded</th>\n",
       "      <th>headquarters</th>\n",
       "      <th>company_name</th>\n",
       "      <th>sector</th>\n",
       "      <th>revenue</th>\n",
       "      <th>rating</th>\n",
       "      <th>type_of_ownership</th>\n",
       "      <th>industry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>270</td>\n",
       "      <td>data_engineer</td>\n",
       "      <td>90-170</td>\n",
       "      <td>santa clara, ca</td>\n",
       "      <td>51 to 200 employees</td>\n",
       "      <td>2011</td>\n",
       "      <td>mountain view, ca</td>\n",
       "      <td>shape security</td>\n",
       "      <td>information technology</td>\n",
       "      <td>unknown_non_applicable</td>\n",
       "      <td>4.1</td>\n",
       "      <td>company_private</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645</th>\n",
       "      <td>645</td>\n",
       "      <td>machine_learning_engineer</td>\n",
       "      <td>90-170</td>\n",
       "      <td>santa clara, ca</td>\n",
       "      <td>10000+ employees</td>\n",
       "      <td>1976</td>\n",
       "      <td>cupertino, ca</td>\n",
       "      <td>apple</td>\n",
       "      <td>information technology</td>\n",
       "      <td>$10+ billion</td>\n",
       "      <td>4.1</td>\n",
       "      <td>company_public</td>\n",
       "      <td>computer_hardware_software</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>620</th>\n",
       "      <td>620</td>\n",
       "      <td>data_scientist</td>\n",
       "      <td>75-145</td>\n",
       "      <td>united states</td>\n",
       "      <td>5001 to 10000 employees</td>\n",
       "      <td>1951</td>\n",
       "      <td>san diego, ca</td>\n",
       "      <td>cubic</td>\n",
       "      <td>aerospace_&amp;_defense</td>\n",
       "      <td>$1 to $2 billion</td>\n",
       "      <td>3.3</td>\n",
       "      <td>company_public</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     index                  job_title salary_range_$_K         location  \\\n",
       "270    270              data_engineer           90-170  santa clara, ca   \n",
       "645    645  machine_learning_engineer           90-170  santa clara, ca   \n",
       "620    620             data_scientist           75-145    united states   \n",
       "\n",
       "                        size  founded       headquarters    company_name  \\\n",
       "270      51 to 200 employees     2011  mountain view, ca  shape security   \n",
       "645         10000+ employees     1976      cupertino, ca           apple   \n",
       "620  5001 to 10000 employees     1951      san diego, ca           cubic   \n",
       "\n",
       "                     sector                 revenue  rating type_of_ownership  \\\n",
       "270  information technology  unknown_non_applicable     4.1   company_private   \n",
       "645  information technology           $10+ billion      4.1    company_public   \n",
       "620     aerospace_&_defense       $1 to $2 billion      3.3    company_public   \n",
       "\n",
       "                       industry  \n",
       "270                      others  \n",
       "645  computer_hardware_software  \n",
       "620                      others  "
      ]
     },
     "execution_count": 500,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_column_names = {\n",
    "    'index': 'index',\n",
    "    'job_cleaned': 'job_title',\n",
    "    'sal_cleaned': 'salary_range_$_K',\n",
    "    'location_cleaned': 'location',\n",
    "    'size_cleaned': 'size',\n",
    "    'founded_clean':'founded',\n",
    "    'headquarters_cleaned': 'headquarters',\n",
    "    'company_name_cleaned': 'company_name',\n",
    "    'sector_cleaned': 'sector',\n",
    "    'revenue_cleaned': 'revenue',\n",
    "    'rating_cleaned': 'rating',\n",
    "    'type_of_ownership_cleaned': 'type_of_ownership',\n",
    "    'indu_cl': 'industry',    \n",
    "}\n",
    "\n",
    "# Renaming the columns and their reorder in the final dataset\n",
    "df_final = df_final.rename(columns=new_column_names)\n",
    "df_final = df_final[new_column_names.values()]\n",
    "df_final.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "id": "acbc39a1-fe56-425c-8ac1-afd2c0af6c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 672 entries, 0 to 671\n",
      "Data columns (total 13 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   index              672 non-null    int64  \n",
      " 1   job_title          672 non-null    object \n",
      " 2   salary_range_$_K   672 non-null    object \n",
      " 3   location           672 non-null    object \n",
      " 4   size               672 non-null    object \n",
      " 5   founded            672 non-null    int64  \n",
      " 6   headquarters       672 non-null    object \n",
      " 7   company_name       672 non-null    object \n",
      " 8   sector             672 non-null    object \n",
      " 9   revenue            672 non-null    object \n",
      " 10  rating             622 non-null    float64\n",
      " 11  type_of_ownership  672 non-null    object \n",
      " 12  industry           672 non-null    object \n",
      "dtypes: float64(1), int64(2), object(10)\n",
      "memory usage: 68.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df_final.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05afa8c0-fccb-48c4-93de-e4ed4db47dbb",
   "metadata": {},
   "source": [
    "#\n",
    "### Data Analysis Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c7ba7a-070c-4e0b-b80e-bf077c47d355",
   "metadata": {},
   "source": [
    "This analysis intends to to provide a comprehensive snapshot of the current Data Science job market, using the job postings on Glassdoor as the case study.\n",
    "The primary aim is to uncover key insights into the job market and explore aspects such as salary estimates, company sizes,  revenue.It aims for find some pattern to answer the follwing questions: \n",
    "- Are there certain industries or sectors with high demand for data science roles? and what are the most common job titles required?\n",
    "- Is there a correlation between company size, revenue, and salary estimation?\n",
    "- Are there geographic regions where the open positions are more prevalent?\n",
    "- and wheatehr any factors that contribute to higher job ratings can be identified?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4805febf-d188-48dd-99d5-62d04f9561d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ironhack_project1_env",
   "language": "python",
   "name": "ironhack_project1_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
